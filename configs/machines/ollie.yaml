# THE ollie.awi.de YAML config file

name: ollie

accounting: false
batch_system: "slurm"

operating_system: "linux-centos"

jobtype: compute
sh_interpreter: "/usr/bin/bash"


choose_jobtype:
        post:
                partition: smp
        compute:
                partition: mpp
        tidy_and_resubmit:
                partition: mpp

choose_partition:
        "mpp":
                cores_per_node: 36
        "smp":
                cores_per_node: 1

logical_cpus_per_core: 2

threads_per_core: 1

pool_dir: "/work/ollie/pool"

pool_directories:
        pool: "/work/ollie/pool"
        projects: "/work/ollie/projects"
        focipool: "/dev/null"

# ?????
choose_submitted:
        true:
                modules_needed:
                        - centoslibs
        false:
                nothing: much

submitted: false
hyper_flag: "" # No hyperthreading on ollie

# standard environment setup
#
#


#Hey Paul,
#please put that into pism.yaml:
#- "load pism_externals"
#- "I_MPI_FABRICS=shm:tmi"

choose_general.setup_name:
        mpiesm:
                add_export_vars:
                   # MPIOM vars
                   - 'ARFLAGS=crv'
                   - 'OASIS3MCT_FC_LIB="-L$NETCDFFROOT/lib -lnetcdff"'
                   - 'OASIS3MCTROOT=${model_dir}'
                   - 'CFLAGS="-O2 -DgFortran -std=gnu99 -xHost"'
                   - 'FCFLAGS="-O3 -fpe0 -i4 -fp-model source -fast-transcendentals -no-prec-sqrt -xHost -heap-arrays -convert big_endian -fpp"'
                   - 'FFLAGS="$FCFLAGS"'
                   - 'WLFLAG="-Wl"'
                   - 'LIBS="-Wl,-rpath,$NETCDF_DIR/lib:$NETCDF_DIR/lib:$HDF5ROOT/lib:""/lib:${I_MPI_ROOT}/intel64/lib"'


useMPI: intelmpi

module_actions:
        - "purge"
        - "load gnu.compiler/6.3.0"
        - "load cmake"
        - "load udunits"
        - "load gribapi/1.28.0"
        - "unload intel.compiler intel.mpi && module load intel.mpi"
        - "unload netcdf"
        - "load centoslibs cdo nco netcdf/4.6.2_gnu"
        # NOTE(PG): Using this model breaks ECHAM6-Concurrent Rad compilation
        # - "load python3/3.7.7_intel2020"
        #- "load automake"
        - load python3/3.6.5_intel2019
        - "list"

export_vars:
        - 'XML2ROOT=/usr'
        - 'PATH=/work/ollie/jhegewal/sw/cmake/bin:$PATH:/home/ollie/jstreffi/fcm/bin/'
        - 'ZLIBROOT=/usr'
        - 'MPIFC=mpif90'
        - 'MPICC=mpigcc'
        #- 'NETCDF_DIR="/global/AWIsoft/netcdf/4.6.2_gnu"'
        - 'NETCDFFROOT=$NETCDF_DIR'
        - 'NETCDFROOT=$NETCDF_DIR'
        - 'NETCDF_Fortran_INCLUDE_DIRECTORIES=$NETCDFROOT/include'
        - 'NETCDF_CXX_INCLUDE_DIRECTORIES=$NETCDFROOT/include'
        - 'NETCDF_CXX_LIBRARIES=$NETCDFROOT/lib'
        - 'PERL5LIB=/usr/lib64/perl5'
        - 'LAPACK_LIB="-lmkl_intel_lp64 -lmkl_core -mkl=sequential -lpthread -lm -ldl"'
        - 'LAPACK_LIB_DEFAULT="-L/global/AWIsoft/intel/2018/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential"'
        - 'FC=mpif90'
        - 'F77=mpif77'
        - 'CC=mpigcc'
        - 'CXX=mpigxx'
        # OpenIFS enviroment
        - 'OIFS_GRIB_API_INCLUDE=-I/home/ollie/jstreffi/ecmwf/grib_api/include'
        - 'OIFS_GRIB_API_LIB="-L/home/ollie/jstreffi/ecmwf/grib_api/lib -lgrib_api_f90 -lgrib_api"'
        - 'OIFS_GRIB_API_BIN="/home/ollie/jstreffi/ecmwf/grib_api/bin"'
        - 'OIFS_NETCDF_LIB="-L$NETCDF_DIR/lib -lnetcdf"' 
        - 'OIFS_NETCDFF_LIB="-L$NETCDF_DIR/lib -lnetcdff"'
        - 'OIFS_FC=$FC'
        - 'OIFS_FFLAGS="-r8 -fp-model precise -align array32byte -O3 -xCORE_AVX2 -g -traceback -qopt-report=0 -convert big_endian"'
        - 'OIFS_FFIXED="-fixed"'
        - 'OIFS_FCDEFS="BLAS LITTLE LINUX INTEGER_IS_INT"'
        # OIFS_MPI_LIB is not defined
        - 'OIFS_LFLAGS=$OIFS_MPI_LIB'
        - 'OIFS_CC=$CC'
        - 'OIFS_CFLAGS="-fp-model precise -O3 -xCORE_AVX2 -g -traceback"'
        - 'OIFS_CCDEFS="LINUX LITTLE INTEGER_IS_INT _ABI64 BLAS"'

choose_useMPI:
        intelmpi:
                add_module_actions:
                        - "unload intel.mpi"
                        - "load intel.mpi"
                fc: mpif90
                f77: mpif90
                mpifc: mpif90
                mpifc: mpigcc
                mpicc: mpigxx
                cc: mpigcc
                cxx: mpigxx
                #fc: '"mpiifort -mkl"'
                #f77: '"mpiifort -mkl"'
                #mpifc: mpiifort
                #mpicc: mpiicc
                #cc: mpiicc
                #cxx: mpiicpc

                #add_export_vars:
                #        - 'MPIROOT=${I_MPI_ROOT}/intel64'

further_reading:
        - batch_system/slurm.yaml
